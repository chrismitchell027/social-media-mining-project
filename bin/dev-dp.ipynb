{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db83b90",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f0db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97faae79",
   "metadata": {},
   "source": [
    "Download Data if Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9180f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_download(download=True, verbose=False):\n",
    "\n",
    "    def print_message(message):\n",
    "        if verbose:\n",
    "            print(message)\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def suppress_stdout():\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            old_stdout = sys.stdout\n",
    "            sys.stdout = devnull\n",
    "            try:\n",
    "                yield\n",
    "            finally:\n",
    "                sys.stdout = old_stdout\n",
    "\n",
    "    target_files = ['../data/hashtag_joebiden.csv', '../data/hashtag_donaldtrump.csv']\n",
    "    missing_files = [file for file in target_files if not os.path.isfile(file)]\n",
    "\n",
    "    if not missing_files:\n",
    "        print_message(\"Both CSV files are present in the current directory.\")\n",
    "        return True, target_files[0], target_files[1]\n",
    "    \n",
    "    \n",
    "    print_message(f\"Missing files detected: {missing_files}\")\n",
    "\n",
    "    if not download:\n",
    "        print_message(\"Data files not detected.\")\n",
    "        return False, None, None\n",
    "    \n",
    "\n",
    "    print_message(\"Data files not detected.\")\n",
    "    print_message(\"Initializing Kaggle API...\")\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "    except OSError:\n",
    "        print(\"Download Failed: Could not find kaggle.json. Make sure it's located in C:/Users/<username>/.kaggle or download it from Kaggle: Your Profile > Settings > API > Create New API Token.\")\n",
    "        return False, None, None\n",
    "\n",
    "    dataset = 'manchunhui/us-election-2020-tweets'\n",
    "    download_path = '../data/'\n",
    "\n",
    "    print_message(f\"Downloading dataset '{dataset}'...\")\n",
    "    if not verbose:\n",
    "        with suppress_stdout():\n",
    "            api.dataset_download_files(dataset, path=download_path, unzip=False)\n",
    "    else:\n",
    "        api.dataset_download_files(dataset, path=download_path, unzip=False)\n",
    "    print_message(\"Download complete.\")\n",
    "\n",
    "    zip_file_path = os.path.join(download_path, 'us-election-2020-tweets.zip')\n",
    "    print_message(f\"Extracting '{zip_file_path}'...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(download_path)\n",
    "    print_message(\"Extraction complete.\")\n",
    "\n",
    "    os.remove(zip_file_path)\n",
    "    print_message(f\"Removed the zip file '{zip_file_path}'.\")\n",
    "\n",
    "    for file in missing_files:\n",
    "        if os.path.isfile(file):\n",
    "            print_message(f\"'{file}' has been successfully downloaded and extracted.\")\n",
    "            return True, target_files[0], target_files[1]\n",
    "        else:\n",
    "            print_message(f\"Warning: '{file}' is still missing after extraction.\")\n",
    "            return False, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892facb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, '../data/hashtag_joebiden.csv', '../data/hashtag_donaldtrump.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_and_download(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c192e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\judem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"no\", \"nor\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"hardly\", \"barely\", \"scarcely\", \n",
    "                  \"isn't\", \"couldn't\", \"aren't\", \"isnt\", \"couldnt\", \"arent\", \"doesn't\", \"doesnt\", \"ain't\", \"aint\", \n",
    "                  \"shouldn't\", \"shouldnt\", \"wasn't\", \"wasnt\", \"weren't\", \"werent\", \"wont\", \"won't\"}\n",
    "stop_words = stop_words - negation_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3091f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load and combine CSV datasets ---\n",
    "df_don = pd.read_csv(\"../data/hashtag_donaldtrump.csv\", lineterminator='\\n', parse_dates=True)\n",
    "df_joe = pd.read_csv(\"../data/hashtag_joebiden.csv\", lineterminator='\\n', parse_dates=True)\n",
    "df_tweets = pd.concat([df_don, df_joe], ignore_index=True)\n",
    "df_tweets = df_tweets.drop_duplicates(subset=[\"tweet_id\"])\n",
    "\n",
    "# print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a7b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\judem\\OneDrive\\USF\\Spring 2025\\Software Engineering\\social-media-mining-project\\env\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: organizePolygons() received a polygon with more than 100 parts.  The processing may be really slow.  You can skip the processing by setting METHOD=SKIP.\n",
      "  return ogr_read(\n",
      "C:\\Users\\judem\\AppData\\Local\\Temp\\ipykernel_30436\\868538724.py:10: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  mask_geo = gdf_tweets.within(us_states.unary_union)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Create a new DataFrame with tweets located in the United States ---\n",
    "us_states = gpd.read_file(\"../data/cb_2020_us_all_500k.gdb\", layer=\"cb_2020_us_state_500k\")\n",
    "us_states = us_states.to_crs(\"EPSG:4326\")\n",
    "df = df_tweets.copy()\n",
    "\n",
    "geo_df = df.dropna(subset=[\"lat\", \"long\"]).copy()\n",
    "geo_df[\"geometry\"] = [Point(xy) for xy in zip(geo_df[\"long\"], geo_df[\"lat\"])]\n",
    "gdf_tweets = gpd.GeoDataFrame(geo_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "mask_geo = gdf_tweets.within(us_states.unary_union)\n",
    "geo_us = gdf_tweets[mask_geo]\n",
    "\n",
    "us_keywords = [\"United States\", \"United States of America\", \"USA\", \"US\"]\n",
    "\n",
    "mask_country = df_tweets[\"country\"].str.contains(\"|\".join(us_keywords), case=False, na=False)\n",
    "mask_user_location = df_tweets[\"user_location\"].str.contains(\"|\".join(us_keywords), case=False, na=False)\n",
    "mask = mask_country | mask_user_location\n",
    "meta_us = df_tweets[mask]\n",
    "\n",
    "df_us = pd.concat([geo_us, meta_us]).drop_duplicates(subset=[\"tweet_id\"])\n",
    "\n",
    "# df_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00e6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    tweet = tweet.lower() # set all characters to lowercase\n",
    "    tweet = re.sub(r'^rt\\s+', '', tweet) # remove RT at start\n",
    "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet) # remove @mentions\n",
    "    tweet = re.sub(r'#', '', tweet) # remove hashtag symbols but keep words\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet) # remove hyperlinks\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet) # reduce character elongations (e.g., loool -> lol, pleeease -> please)\n",
    "    tweet = re.sub(r'[^A-Za-z0-9\\s]', '', tweet) # remove special characters\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii') # remove emojis\n",
    "    # tweet = re.sub(r'\\d+', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip() # remove extra spaces\n",
    "    \n",
    "    # # remove stopwords (e.g., the, is, at, which, on, and, etc.)\n",
    "    # tweet_tokens = tweet.split()\n",
    "    # filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "    # tweet = \" \".join(filtered_words)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c85b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_us.copy()\n",
    "df_cleaned[\"clean_tweet\"] = df_cleaned[\"tweet\"].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243005f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"tweet_id\", \"clean_tweet\", \"user_id\", \"lat\", \"long\", \"city\", \"country\", \"state\", \"state_code\"]\n",
    "df_cleaned = df_cleaned[columns_to_keep]\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"tweet_id\", \"clean_tweet\"])\n",
    "df_cleaned.to_csv(\"../data/cleaned_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f223c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(\"../data/cleaned_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18bbfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "len(df_cleaned) # 404205 tweets in the cleaned dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
